{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15afe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625808e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import evallm\n",
    "from automata.fa.dfa import DFA\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294c9a50-c317-4a8a-9396-afa17e1e26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from permacache import stable_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fe887b-d30f-4c00-acef-3c7a4902ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evallm.llm.llm import model_specs\n",
    "from evallm.prompting.transducer_prompt import (\n",
    "    ChainOfThoughtPromptRealExampleNoExplanation,\n",
    "    BasicSequencePrompt,\n",
    "    BasicSequencePromptSlightlyMoreExplanation,\n",
    "    BasicSequencePromptNoChat,\n",
    "    SequencePromptWithExplanation,\n",
    "    SequencePromptWithExplanationChainOfThought,\n",
    "    RedGreenRoomPrompt1,\n",
    "    BasicInstructionTransducerPrompter,\n",
    ")\n",
    "from evallm.experiments.transducer_experiment import (\n",
    "    current_transducer_experiments,\n",
    "    compute_relative_to_null,\n",
    "    compute_relative_to_ngram,\n",
    "    print_example,\n",
    "    bottom_quartile_outcome,\n",
    "    current_dfa_sample_spec,\n",
    "    run_transducer_experiment_just_stats,\n",
    "    run_transducer_experiment,\n",
    ")\n",
    "from evallm.experiments.transducer_plotting import (\n",
    "    plot_all_absolute_results_single_graph,\n",
    "    plot_absolute_results_barchart,\n",
    ")\n",
    "from evallm.utils.bootstrap import boostrap_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373f5e4b-31cd-41f2-820a-1d840b459c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 3\n",
    "num_sequence_symbols = 30\n",
    "num_repeats_per_dfa = 30\n",
    "sample_dfa_spec = current_dfa_sample_spec(num_states=num_states)\n",
    "setting_kwargs = dict(\n",
    "    num_sequence_symbols=num_sequence_symbols,\n",
    "    sample_dfa_spec=sample_dfa_spec,\n",
    "    num_states=num_states,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8facd648-8323-414f-8965-7b2598aaac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_by_key = {\n",
    "    \"llama3-8B\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"llama3-70B\": \"meta-llama/Meta-Llama-3-70B\",\n",
    "    \"llama3.1-8B-Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistral-nemo-minitron-8B\": \"nvidia/Mistral-NeMo-Minitron-8B-Base\",\n",
    "    \"mistral-nemo-base-12B\": \"/scratch/kavig/mistral_models/Nemo-Base\",\n",
    "    \"mistral-nemo-instruct-12B\": \"/scratch/kavig/mistral_models/Nemo-Instruct\",\n",
    "    \"gemma-7b\": \"google/gemma-7b\",\n",
    "    \"falcon-7b\": \"tiiuae/falcon-7b\",\n",
    "    \"gpt-3.5-instruct\": \"gpt-3.5-turbo-instruct\",\n",
    "    \"gpt-3.5-chat\": \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4o\": \"gpt-4o-2024-05-13\",\n",
    "    \"claude-3.5\": \"claude-3-5-sonnet-20241022\",\n",
    "}\n",
    "prompt_by_key = {\n",
    "    \"Basic\": {\n",
    "        \"non-chat\": BasicSequencePromptNoChat.for_setting(setting_kwargs),\n",
    "        \"chat\": BasicSequencePrompt.for_setting(setting_kwargs),\n",
    "    },\n",
    "    \"More-Expl\": {\n",
    "        \"chat\": BasicSequencePromptSlightlyMoreExplanation.for_setting(setting_kwargs)\n",
    "    },\n",
    "    # \"full-expl\": {\"chat\": SequencePromptWithExplanation.for_setting(setting_kwargs)},\n",
    "    \"COT\": {\n",
    "        \"chat\": SequencePromptWithExplanationChainOfThought.for_setting(setting_kwargs)\n",
    "    },\n",
    "    \"Red-Green\": {\"chat\": RedGreenRoomPrompt1.for_setting(setting_kwargs)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce74b2a-43e1-4097-8792-6635dc780c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_model_and_prompt(model, num_dfas, *prompts):\n",
    "    model_key = model_by_key[model]\n",
    "    if model_specs[model_key].is_chat:\n",
    "        prompt_kind = \"chat\"\n",
    "    else:\n",
    "        prompt_kind = \"non-chat\"\n",
    "    return {\n",
    "        (model, prompt): run_transducer_experiment(\n",
    "            model_key,\n",
    "            sample_dfa_spec,\n",
    "            prompt_by_key[prompt][prompt_kind],\n",
    "            num_repeats_per_dfa=num_repeats_per_dfa,\n",
    "            num_dfas=num_dfas,\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c759712b-8543-46cb-a443-11a1a4b0abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_baseline_outcomes = run_transducer_experiment_just_stats(\n",
    "    \"none\",\n",
    "    sample_dfa_spec,\n",
    "    BasicInstructionTransducerPrompter(num_sequence_symbols, strip=True),\n",
    "    num_repeats_per_dfa=num_repeats_per_dfa,\n",
    "    num_dfas=1000,\n",
    ")\n",
    "model_outcomes = {\n",
    "    **for_model_and_prompt(\"llama3-8B\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"llama3-70B\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"llama3.1-8B-Instruct\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"llama3.1-8B-Instruct\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"mistral-nemo-minitron-8B\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"mistral-nemo-base-12B\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"mistral-nemo-instruct-12B\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"gemma-7b\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"falcon-7b\", 1000, \"Basic\"),\n",
    "    **for_model_and_prompt(\"gpt-3.5-instruct\", 100, \"Basic\"),\n",
    "    **for_model_and_prompt(\"gpt-3.5-chat\", 100, \"Basic\"),\n",
    "    **for_model_and_prompt(\n",
    "        \"gpt-4o-mini\",\n",
    "        100,\n",
    "        \"Basic\",\n",
    "        \"More-Expl\",\n",
    "        # \"full-expl\",\n",
    "        \"COT\",\n",
    "        \"Red-Green\",\n",
    "    ),\n",
    "    **for_model_and_prompt(\"gpt-4o\", 30, \"Basic\"),\n",
    "    **for_model_and_prompt(\n",
    "        \"claude-3.5\",\n",
    "        30,\n",
    "        \"Basic\",\n",
    "        \"More-Expl\",\n",
    "        \"COT\",\n",
    "        \"Red-Green\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23b3e7f-3e11-497b-992f-9aaafc055a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_prompt = \"Basic\"\n",
    "\n",
    "accuracies = defaultdict(dict)\n",
    "accuracies[r\"\\textsc{Null}\"][no_prompt] = [\n",
    "    r.null_success_rate for r in deterministic_baseline_outcomes\n",
    "]\n",
    "for ngram in range(2, 2 + 5):\n",
    "    accuracies[rf\"{ngram}-\\textsc{{Gram}}\"][no_prompt] = [\n",
    "        r.kgram_success_rates_each[ngram - 2] for r in deterministic_baseline_outcomes\n",
    "    ]\n",
    "accuracies[r\"\\textsc{BruteForce}\"][no_prompt] = [\n",
    "    r.brute_force_inference for r in deterministic_baseline_outcomes\n",
    "]\n",
    "for model, prompt in model_outcomes:\n",
    "    accuracies[model][prompt] = [\n",
    "        r.success_rate_binary_ignore_na for r in model_outcomes[model, prompt]\n",
    "    ]\n",
    "\n",
    "accuracies_mean = {\n",
    "    mod: {prompt: np.mean(accuracies[mod][prompt]) for prompt in accuracies[mod]}\n",
    "    for mod in accuracies\n",
    "}\n",
    "best_acc_mean_by_mod = {k: max(v.values()) for k, v in accuracies_mean.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f7f83b-f9ae-47ed-afe4-bf108f33ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(prompt_by_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36326e6c-108f-42dc-86c2-a88ea38883f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_acc(acc, mod):\n",
    "    acc = np.array(acc) * 100\n",
    "    mu = np.mean(acc)\n",
    "    lo, hi = boostrap_mean(acc)\n",
    "    prefix = format_by_mod.get(mod, \"\")\n",
    "    return prefix + f\"{mu:.1f} ({lo:.1f}--{hi:.1f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c548b783-49ee-4505-8fb0-8c09d1ae51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sorted = sorted(accuracies, key=lambda k: best_acc_mean_by_mod[k])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0825c5db-783c-4492-8827-0b381bead5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_by_mod = {}\n",
    "assert \"BruteForce\" in models_sorted[0]\n",
    "format_by_mod[models_sorted[0]] = r\"\\cellcolor{lightgray}\"\n",
    "format_by_mod[models_sorted[1]] = r\"\\bf \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee9a302-2ac8-4949-b67e-bc1caf59f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|r|c|c|c|c|}\n",
      "\\hline\n",
      "Model & \\textsc{Basic} & \\textsc{More-Expl} & \\textsc{COT} & \\textsc{Red-Green}\\\\\n",
      "\\hline\n",
      "\\cellcolor{lightgray}\\textsc{BruteForce} &\\cellcolor{lightgray}96.0 (95.8--96.3)&--&--&--\\\\\n",
      "\\hline\n",
      "\\bf 6-\\textsc{Gram} &\\bf 93.5 (93.1--93.9)&--&--&--\\\\\n",
      "\\hline\n",
      "5-\\textsc{Gram} &93.4 (93.0--93.7)&--&--&--\\\\\n",
      "\\hline\n",
      "4-\\textsc{Gram} &91.1 (90.6--91.6)&--&--&--\\\\\n",
      "\\hline\n",
      "mistral-nemo-minitron-8B &88.6 (88.0--89.1)&--&--&--\\\\\n",
      "\\hline\n",
      "mistral-nemo-instruct-12B &88.0 (87.5--88.5)&--&--&--\\\\\n",
      "\\hline\n",
      "mistral-nemo-base-12B &87.9 (87.4--88.4)&--&--&--\\\\\n",
      "\\hline\n",
      "gpt-3.5-instruct &87.8 (85.9--89.6)&--&--&--\\\\\n",
      "\\hline\n",
      "llama3-70B &87.7 (87.2--88.3)&--&--&--\\\\\n",
      "\\hline\n",
      "llama3-8B &87.5 (86.9--88.0)&--&--&--\\\\\n",
      "\\hline\n",
      "claude-3.5 &86.9 (83.3--90.0)&87.1 (83.9--90.2)&73.2 (68.8--77.4)&82.9 (78.9--86.9)\\\\\n",
      "\\hline\n",
      "3-\\textsc{Gram} &87.0 (86.4--87.6)&--&--&--\\\\\n",
      "\\hline\n",
      "llama3.1-8B-Instruct &85.9 (85.3--86.5)&--&--&--\\\\\n",
      "\\hline\n",
      "falcon-7b &84.9 (84.3--85.5)&--&--&--\\\\\n",
      "\\hline\n",
      "gpt-4o &83.7 (80.1--86.9)&--&--&--\\\\\n",
      "\\hline\n",
      "gemma-7b &82.1 (81.4--82.7)&--&--&--\\\\\n",
      "\\hline\n",
      "gpt-4o-mini &79.8 (77.3--82.2)&76.7 (74.2--79.3)&66.4 (64.4--68.6)&74.5 (72.0--77.0)\\\\\n",
      "\\hline\n",
      "2-\\textsc{Gram} &74.5 (73.6--75.3)&--&--&--\\\\\n",
      "\\hline\n",
      "\\textsc{Null} &68.9 (68.2--69.6)&--&--&--\\\\\n",
      "\\hline\n",
      "gpt-3.5-chat &66.8 (63.4--69.8)&--&--&--\\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "table_alignments = \"|\" + \"|\".join(\"r\" + \"c\" * len(prompts)) + \"|\"\n",
    "table = \"\"\n",
    "table += r\"\\begin{tabular}{%s}\" % table_alignments + \"\\n\"\n",
    "table += r\"\\hline\" + \"\\n\"\n",
    "table += \" & \".join([\"Model\"] + [rf\"\\textsc{{{x}}}\" for x in prompts]) + r\"\\\\\" + \"\\n\"\n",
    "table += r\"\\hline\" + \"\\n\"\n",
    "for mod in models_sorted:\n",
    "    table += format_by_mod.get(mod, \"\") + mod + \" &\"\n",
    "    if \"no-prompt\" in accuracies[mod]:\n",
    "        table += (\n",
    "            r\"\\multicolumn{%s}{l|}{%s}\"\n",
    "            % (\n",
    "                len(prompts),\n",
    "                display_acc(accuracies[mod][\"no-prompt\"], mod),\n",
    "            )\n",
    "            + r\"\\\\\"\n",
    "            + \"\\n\"\n",
    "        )\n",
    "        table += r\"\\hline\" + \"\\n\"\n",
    "        continue\n",
    "    for prompt in prompts:\n",
    "        # if prompt == \"no-prompt\":\n",
    "        #     prompt = \"basic\"\n",
    "        if prompt not in accuracies[mod]:\n",
    "            table += \"--&\"\n",
    "            continue\n",
    "        table += display_acc(accuracies[mod][prompt], mod) + \"&\"\n",
    "    assert table[-1] == \"&\"\n",
    "    table = table[:-1]\n",
    "    table += r\"\\\\\" + \"\\n\"\n",
    "    table += r\"\\hline\" + \"\\n\"\n",
    "table += r\"\\end{tabular}\"\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe376493-c6b4-412e-8d46-e8749c109cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
